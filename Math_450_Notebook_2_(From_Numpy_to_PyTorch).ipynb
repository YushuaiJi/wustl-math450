{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Math 450 Notebook 2 (From Numpy to PyTorch).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNPbNXwkO4s4me4u3O5TJ35",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scaomath/wustl-math450/blob/main/Math_450_Notebook_2_(From_Numpy_to_PyTorch).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9nfcOnJw7VB"
      },
      "source": [
        "# Coding Lecture 2 of Math 450\n",
        "\n",
        "Goal of the our class: make us learn machine learning in torch package.\n",
        "- Build our own neural net using Torch's LEGO-like blocks.\n",
        "- Write testing code from scratch.\n",
        "- Write our own optimizer.\n",
        "\n",
        "\n",
        "This is a worksheet version of the notebook. We can follow along during the coding lecture and then download the annotated version in our Github repository.\n",
        "\n",
        "Download this notebook at:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WQozZ69ytx-"
      },
      "source": [
        "import numpy as np\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ga29qYH3RNg"
      },
      "source": [
        "## Review of Coding Lecture 1: From Numpy to Torch\n",
        "\n",
        "- Matrix vector multiplication (`dot` in `numpy` and `mm` in `torch`) vs `*` (elementwise multiplication)\n",
        "- Axes of an array, `squeeze()`.\n",
        "- Object-oriented way of applying functions.\n",
        "- `reshape()` in `numpy` vs `view()` in torch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s68fUq33Qf1"
      },
      "source": [
        "# matrix vector multiplication vs *\n",
        "x = np.array([[1,2], [0,5]])\n",
        "y = np.array([1.3, 2.5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOj-LHY9zoPU"
      },
      "source": [
        "# Key components for a neural network\n",
        "\n",
        "\n",
        "## Multi-layer, multiple perceptrons per layer\n",
        "If we have $m$ perceptrons in a single layer, for example layer 2:\n",
        "<img src=\"https://sites.wustl.edu/scao/files/2021/02/neural_net_3-layer.png\" alt=\"drawing\" width=\"800\"/>\n",
        "\n",
        "Our neural network has parameters $(W, b) := \\big(W^{(1)},b^{(1)},W^{(2)},b^{(2)}\\big)$.\n",
        "\n",
        "* $W^{(l)} = \\big(w^{(l)}_{ij}\\big)$ to denote the weight matrix, where the entry-$ij$ is associated with the connection between unit $j$ in layer $l$, and unit $i$ in layer $l+1$. Note the order of the indices, $j$ is the closer to the input that this matrix is acting on \n",
        "\n",
        "* $b^{(l)}_i$ is the bias associated with unit $i$ in layer $l+1$. \n",
        "\n",
        "In our example above, we have $W^{(1)}\\in \\mathbb{R}^{3×2}$, and $W^{(2)}\\in \\mathbb{R}^{1×3}$. Note that bias units do not have inputs or connections going into them, we write their output the value $+1$ for convenience. When we count the number of units in layer $l$, we do not count the bias unit.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVs70GYD0fcj"
      },
      "source": [
        "## Matrix-vector representation\n",
        "If we allow the activation function $f(\\cdot)$ to act on vectors in an element-wise fashion: $f([\\mathbf{z}_1,\\mathbf{z}_2,\\mathbf{z}_3])=[f(\\mathbf{z}_1),f(\\mathbf{z}_3),f(\\mathbf{z}_3)]$, then we can write the equations above more compactly as:\n",
        "$$\\begin{aligned}\n",
        "\\mathbf{z}^{(2)} &= W^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)} \\\\\n",
        "\\mathbf{a}^{(2)} &= f(\\mathbf{z}^{(2)}) \\\\\n",
        "\\mathbf{z}^{(3)} &= W^{(2)} \\mathbf{a}^{(2)} + \\mathbf{b}^{(2)} \\\\\n",
        "h(\\mathbf{x}; W, b) &= \\mathbf{a}^{(3)} = \\mathbf{z}^{(3)}\n",
        "\\end{aligned}\n",
        "$$\n",
        "More generally, if we have an arbitrary number of layers, recalling that $\\mathbf{a}^{(0)}=\\mathbf{x}$ also denotes the values from the input layer, then given layer $l$'s activations $\\mathbf{a}^{(l)}$, we can compute layer $(l+1)$'s activations $\\mathbf{a}^{(l+1)}$ as:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{z}^{(l+1)} &= W^{(l)} \\mathbf{a}^{(l)} + \\mathbf{b}^{(l)}   \\\\\n",
        "\\mathbf{a}^{(l+1)} &= f(\\mathbf{z}^{(l+1)}),\n",
        "\\end{aligned}\n",
        "$$\n",
        "except the last layer where we do not need any activation.\n",
        "By organizing the parameters in matrices and using matrix-vector operations, we can take advantage of fast linear algebra routines to quickly perform calculations in our network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g52kI59TyvX5"
      },
      "source": [
        "# generate all the x, z, a variables above"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUS2o3NG3Ko2"
      },
      "source": [
        "# from input to first layer (hidden layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtD7pojv5P7z"
      },
      "source": [
        "# from the first layer to the output layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilVYNAK-7P_v"
      },
      "source": [
        "## Actual data\n",
        "\n",
        "In the actual implementation, the data normaly comes in batch, i.e., a matrix. For example, input is a matrix $X \\in \\mathbb{R}^{N \\times d}$, $N$ is a number of samples in a batch, each row represents a sample $\\mathbf{x} \\in \\mathbb{R}^{1\\times d}$. The weight matrix $W$ is actually formulated as:\n",
        "$$\n",
        "W = \\left(\n",
        "\\begin{array}{cccc}| & | & | & | \\\\\n",
        "\\mathbf{w}_1 & \\mathbf{w}_2 & \\cdots & \\mathbf{w}_m \\\\\n",
        "| & | & | & |\n",
        "\\end{array}\\right),\n",
        "$$\n",
        "if the output dimension of the layer of interest is $m$. The vectorized formulation is, for example, from the input (layer 0, dimension $d$) to layer 1 (dimension $m$)\n",
        "$$\n",
        "A^{(1)} = X W^{(0)} + B\n",
        "$$\n",
        "where $X \\in \\mathbb{R}^{N \\times d}$, $W^{(0)} \\in \\mathbb{R}^{d\\times m}$ (input from $d$ perceptrons, output from $m$ perceptrons), $B$ is a matrix with each row being the same $\\mathbf{b} \\in \\mathbb{R}^{1\\times m}$ (layer 1 has $m$ perceptrons and has $m$ biases if applicable).\n",
        "\n",
        "## Torch's nn module\n",
        "\n",
        "We will demo this batch-based operation using `torch`'s neural network module `nn`. `nn.Linear` applies an (affine) linear transformation to the incoming data:\n",
        "$$\n",
        "Y = X W^{\\top} + \\mathbf{b}\n",
        "$$\n",
        "\n",
        "Reference: https://pytorch.org/docs/stable/nn.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbvdVRdM6r64"
      },
      "source": [
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNkIeejp6uX_"
      },
      "source": [
        "# Linear layer example\n",
        "layer = nn.Linear(4, 3)\n",
        "input = torch.randn(32, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmhubnX-5ywT"
      },
      "source": [
        "# MNIST\n",
        "\n",
        "\"MNIST (\"Modified National Institute of Standards and Technology\") is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.\"\n",
        "\n",
        "[Read more.](https://www.kaggle.com/c/digit-recognizer)\n",
        "\n",
        "\n",
        "<a title=\"By Josef Steppan [CC BY-SA 4.0 (https://creativecommons.org/licenses/by-sa/4.0)], from Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:MnistExamples.png\"><img width=\"512\" alt=\"MnistExamples\" src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\"/></a>\n",
        "\n",
        "This code is adopted from the pytorch examples repository. \n",
        "It is licensed under BSD 3-Clause \"New\" or \"Revised\" License.\n",
        "Source: https://github.com/pytorch/examples/\n",
        "LICENSE: https://github.com/pytorch/examples/blob/master/LICENSE\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVC2HKSH79Kx"
      },
      "source": [
        "from __future__ import print_function\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQ--T0wp8Mzz"
      },
      "source": [
        "train = datasets.MNIST('../data', train=True, download=True, transform = transforms.ToTensor())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxyZFSpU8aD_"
      },
      "source": [
        "train_loader = DataLoader(train, batch_size=1, shuffle=True, num_workers=2,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hZIuUgO82se"
      },
      "source": [
        "data_iter = iter(train_loader)\n",
        "images, labels = next(data_iter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2PTbAit9N65"
      },
      "source": [
        "im = make_grid(images)\n",
        "plt.imshow(np.transpose(im.numpy(), (1, 2, 0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_sxp2G39zFG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}